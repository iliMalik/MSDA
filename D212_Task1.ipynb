{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adec327",
   "metadata": {},
   "source": [
    "# D212 Data Mining II Performance Assessment, Task-1\n",
    "\n",
    "Submitted by Muhammad Ilyas, Student ID 011143032, for WGU's MSDA program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0d203-5f7b-414e-8c77-43096ee97918",
   "metadata": {},
   "source": [
    "## Part I: Research Question\n",
    "### A1: Proposal of Question\n",
    "\n",
    "\"Can we identify distinct patient groups based on their demographic information and medical conditions to tailor specific treatment strategies or programs for each group, ultimately aiming to improve patient outcomes and reduce readmission rates?\"\n",
    "\n",
    "This question focuses on leveraging hierarchical clustering to segment patients into meaningful groups that share similar characteristics. By identifying these clusters, hospitals can personalize treatment plans, allocate resources more efficiently, and potentially mitigate factors leading to readmissions.\n",
    "### A2: Defined Goal\n",
    "\n",
    "A reasonable goal within the scope of the scenario and the available data could be:\n",
    "\n",
    "\"Identify distinct patient clusters based on demographic details, medical conditions, and hospital interaction patterns to create targeted patient care programs aimed at reducing readmission rates and improving overall patient satisfaction.\"\n",
    "\n",
    "This goal aligns with the scenario's objective of understanding patient characteristics and utilizes the available data, which includes demographic information, medical conditions, hospital interactions, and patient readmission status. The aim is to leverage hierarchical clustering to segment patients effectively, allowing hospitals to tailor interventions and care plans to specific patient groups for better outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c7582-d1ab-460d-ae67-54eb0a16627c",
   "metadata": {},
   "source": [
    "## Part II: Technique Justification\n",
    "\n",
    "### B1: Explanation of Clustering Technique\n",
    "\n",
    "#### Data Assessment: \n",
    "Hierarchical clustering iteratively merges or splits clusters based on similarities/dissimilarities between data points (patients) across various attributes (demographics, medical conditions, hospital interactions).\n",
    "#### Distance Calculation: \n",
    "It measures distances between data points using chosen metrics (Euclidean, Manhattan, etc.) to determine similarity.\n",
    "#### Hierarchical Structure: \n",
    "The algorithm creates a dendrogram showing relationships between clusters and their members. The expected outcome includes identifying patient clusters with similar characteristics, aiding in segmenting patients into distinct groups based on shared attributes.\n",
    "\n",
    "### B2: Summary of Technique Assumption\n",
    "Hierarchical clustering assumes that the data points being clustered can be represented by a hierarchy. It also assumes that the distance metric used to measure similarity between data points is meaningful and accurately represents their similarities in the given context.\n",
    "\n",
    "### B3: Packages or Libraries List\n",
    "\n",
    "#### scikit-learn: \n",
    "Offers hierarchical clustering algorithms like AgglomerativeClustering. Supports distance calculations and dendrogram visualization. Also, it provides tools for preprocessing data before clustering.\n",
    "#### matplotlib/seaborn: \n",
    "For visualizing dendrograms and cluster distributions, aiding in the interpretation of cluster structures.\n",
    "#### pandas: \n",
    "Essential for data manipulation and preprocessing, enabling data cleaning and transformation before clustering.\n",
    "Each of these libraries supports different aspects of the analysis. Scikit-learn provides the clustering algorithms and tools for distance computation, while matplotlib/seaborn help in visualizing the clusters, and pandas facilitates data preprocessing, ensuring the data is in a suitable format for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from pandas.api.types import CategoricalDtype  # For handling categorical data types\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import seaborn as sns  # For enhanced data visualization\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram  # For hierarchical clustering and dendrogram plotting\n",
    "from sklearn.metrics import silhouette_score  # For evaluating clustering performance\n",
    "\n",
    "# Read the dataset into a DataFrame\n",
    "df = pd.read_csv('./medical_clean.csv', index_col=0)  # Assumes the dataset is in a CSV file named 'medical_clean.csv'\n",
    "df.info()  # Displays information about the DataFrame such as column names, data types, and non-null counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9098b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b24e7-0e1a-4d2c-9b51-4f73d99f4737",
   "metadata": {},
   "source": [
    "## Part III: Data Preparation\n",
    "### C1: Data processing (goal)\n",
    "A relevant preprocessing goal is handling missing values. Clustering algorithms often struggle with missing data. Imputation methods or dropping incomplete rows might be necessary to ensure the clustering algorithm can work effectively with complete data.\n",
    "\n",
    "### C2: Data Variables\n",
    "Here are some variables from the dataset categorized as continuous or categorical:\n",
    "\n",
    "#### Continuous: \n",
    "Age, Income, VitD_levels, Doc_visits, Full_meals_eaten, TotalCharge, Additional_charges, Item1-Item8.\n",
    "#### Categorical: \n",
    "City, State, County, Area, TimeZone, Job, Marital, Gender, ReAdmis, Services, Initial_admin, HighBlood, Stroke, Complication_risk, Overweight, Arthritis, Diabetes, Hyperlipidemia, BackPain, Anxiety, Allergic_rhinitis, Reflux_esophagitis, Asthma, Soft_drink.\n",
    "### C3: Steps for Analysis\n",
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a6556-7c96-48f9-827b-eb67ffe10388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with missing values for important columns\n",
    "important_cols = ['Age', 'Income', 'VitD_levels', 'Doc_visits', 'Full_meals_eaten']\n",
    "cleaned_data = df.dropna(subset=important_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4bd3c-93fc-4ece-b3ee-530d7c2a3f80",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41673fc-1eae-435e-a522-2620d8b2546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas get_dummies for one-hot encoding categorical variables\n",
    "categorical_cols = ['Area', 'TimeZone', 'Job', 'Marital', 'Gender', 'Services']\n",
    "cleaned_data = pd.get_dummies(cleaned_data, columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f2ed3-5651-4cd8-9927-0c83c9f68aa4",
   "metadata": {},
   "source": [
    "#### Scaling Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e850b-d94b-4724-a4f2-4aebbd921da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scaling continuous variables\n",
    "scaler = StandardScaler()\n",
    "continuous_cols = ['Age', 'Income', 'VitD_levels', 'Doc_visits', 'Full_meals_eaten']\n",
    "cleaned_data[continuous_cols] = scaler.fit_transform(cleaned_data[continuous_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f255e1b-be57-474e-ab5f-0cf38b1ffda0",
   "metadata": {},
   "source": [
    "### C4: Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64237d69-2d5e-4b96-a3d8-ce0184fe91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('task1_full_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bb411-c144-4616-b794-5bf911ecc63d",
   "metadata": {},
   "source": [
    "## Part IV: Analysis\n",
    "### D1: Output and Intermediate Calculations\n",
    "\n",
    "I will use following steps:\n",
    "\n",
    "#### Import Libraries:\n",
    "Import the necessary libraries, including matplotlib.pyplot for plotting and scipy.cluster.hierarchy for hierarchical clustering.\n",
    "\n",
    "#### Select Numeric Columns for Clustering:\n",
    "Identify and select only the numeric columns from the dataset to be used for clustering. This is a common practice when applying clustering algorithms.\n",
    "\n",
    "#### Perform Hierarchical Clustering:\n",
    "Use the Ward method to perform hierarchical clustering on the selected numeric data. The result is stored in a linkage matrix.\n",
    "\n",
    "#### Plot the Dendrogram:\n",
    "Create a dendrogram plot based on the hierarchical clustering results. The plot displays the relationships between data points or clusters. Adjustments can be made to focus on a specific number of clusters for better visualization.\n",
    "\n",
    "The overall purpose of this code is to provide insights into the structure and relationships within the numeric data through hierarchical clustering and visualize the results using a dendrogram. The choice of parameters in the dendrogram plot can be adjusted to meet specific analysis requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cedf31d-9007-48ef-be65-099be0f04d21",
   "metadata": {},
   "source": [
    "### D2: Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ceee7a-b91f-4214-9818-4a4eef50a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Selecting only numeric columns for clustering\n",
    "numeric_columns = cleaned_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_data = cleaned_data[numeric_columns]\n",
    "\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linked = linkage(numeric_data, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linked, truncate_mode='lastp', p=50)  # Adjust 'p' for better visualization\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index or Cluster Size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137cd1b-20ba-4211-b71c-b0f420396fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Assuming 'data' is your cleaned and preprocessed DataFrame\n",
    "# Choosing the optimal number of clusters based on the dendrogram analysis\n",
    "\n",
    "# Initialize the clustering model with the identified number of clusters\n",
    "n_clusters = 5\n",
    "model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "\n",
    "# Fit the model to your data\n",
    "clusters = model.fit_predict(numeric_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c169b6-fe01-4004-a5bf-843f7963786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster labels to the original DataFrame\n",
    "data_with_clusters = numeric_data.copy()\n",
    "data_with_clusters['Cluster'] = clusters\n",
    "\n",
    "# Analyze each cluster\n",
    "cluster_means = data_with_clusters.groupby('Cluster').mean()\n",
    "print(cluster_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b129a-ce94-44c5-8c95-d40faeb4ea84",
   "metadata": {},
   "source": [
    "## Part V: Data Summary and Implications\n",
    "### E1: Quality of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2606d3b-7341-4eee-9b29-fc92b5c5ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_avg = silhouette_score(data_with_clusters, clusters)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242345a6-d6fb-45ff-bbd4-a219119d2559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming 'data' contains your data for clustering and 'num_clusters' is the determined number of clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(numeric_data)\n",
    "wcss = kmeans.inertia_\n",
    "print(f\"WCSS: {wcss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb2bad-f325-4918-9263-2327e1b83d4b",
   "metadata": {},
   "source": [
    "A silhouette score of 0.368 indicates a moderate level of separation between the clusters. This score falls within the range of -1 to 1, where values closer to 1 suggest better-defined clusters with distinct boundaries. A score around 0.368 implies that while the clusters are discernible, there might be some overlapping or ambiguity in the assignments. It's not a high separation, but it's indicative of reasonable clustering, especially considering real-world data where perfect separation might not always be achievable.\n",
    "\n",
    "The Within-Cluster Sum of Squares (WCSS) of approximately 2.14 trillion suggests the total sum of squares of distances of data points to their respective cluster centroids. This value helps to assess how compact the clusters are. A lower WCSS generally indicates that the data points within each cluster are closer to the centroid, implying more compact and cohesive clusters. However, the interpretation of the absolute value of WCSS might not provide substantial insights, especially considering the magnitude of the value. Comparing WCSS across different numbers of clusters or different models might be more informative(Schurz, 2023).\n",
    "\n",
    "The clusters exhibit a moderate level of separation, as indicated by a silhouette score of 0.368. This suggests that while the clusters are discernible, there might be some overlap or ambiguity in the assignments. The Within-Cluster Sum of Squares (WCSS) of approximately 2.14 trillion indicates the sum of squares of distances of data points to their respective cluster centroids. This suggests a substantial spread of data points within each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8803fa-5ff8-4fd9-b1d8-b5c87c1b8295",
   "metadata": {},
   "source": [
    "### E2: Results and Implications\n",
    "\n",
    "The clustering analysis has successfully identified distinct groups of patients with similar characteristics. This understanding could be used for various strategic purposes:\n",
    "\n",
    "#### Tailoring treatments: \n",
    "Design specific treatment plans for each cluster based on their unique characteristics.\n",
    "#### Resource allocation: \n",
    "Allocate resources more efficiently by targeting specific patient groups with tailored services.\n",
    "#### Marketing strategies: \n",
    "Develop targeted marketing campaigns for each patient cluster to enhance patient engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd6cb1-b9f7-4731-9169-6fdf9e731edc",
   "metadata": {},
   "source": [
    "### E3: Limitation\n",
    "\n",
    "A limitation could be the moderate silhouette score and the relatively high WCSS, indicating that while clusters are identifiable, they might not be as well-separated or compact as desired. This might affect the precision of tailoring treatments or services for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6df92d-d1f7-450d-87b9-879ba2a02cb7",
   "metadata": {},
   "source": [
    "### E4: Course of Action:\n",
    "Considering the moderate separation of clusters and the relatively large spread of data within clusters, it's recommended to:\n",
    "\n",
    "#### Refine clustering techniques: \n",
    "Experiment with different clustering algorithms or parameters to achieve better-defined and more compact clusters.\n",
    "#### Validate clusters: \n",
    "Use additional validation techniques or include more relevant features to enhance the separation between clusters and improve cluster homogeneity.\n",
    "#### Iterative improvement: \n",
    "Continuously refine the clustering model based on new data or additional insights to better tailor treatments and services for each patient cluster.\n",
    "\n",
    "By refining the clustering model and ensuring better separation between clusters, the hospital can enhance the effectiveness of tailored treatments and optimize resource allocation, thereby improving overall patient care and organizational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba11832-1cfb-43f5-8248-c20c03edac64",
   "metadata": {},
   "source": [
    "### G: Sources for Thirs Party Code\n",
    "No third party code was used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e82669e-c862-46d4-9d1a-5fffa373e33b",
   "metadata": {},
   "source": [
    "### H: Sources\n",
    "James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An introduction to statistical learning: with Applications in Python. Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
