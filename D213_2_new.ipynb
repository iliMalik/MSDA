{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74791710-88aa-4307-a8c4-00ef9a4d7904",
   "metadata": {},
   "source": [
    "# D213 Advanced Data Analytics\n",
    "## Task-2\n",
    "### Submitted by Muhammad Ilyas, Student ID 011143032\n",
    "### WGU's MSDA program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441ab35-7c08-4f66-831f-b1fc446fd936",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part-I: Reasearch Question\n",
    "## A1: Research Question\n",
    "How can sentiment analysis using neural network models and NLP techniques on IMDb reviews contribute to understanding audience reactions and preferences in the movie industry?\n",
    "## A2: Objectives or Goals\n",
    "#### Sentiment Analysis: \n",
    "Utilize neural network models to predict sentiment (positive/negative) of IMDb reviews, providing insights into the overall reception of movies.\n",
    "#### Feature Extraction: \n",
    "Extract meaningful features from text data to enhance the model's understanding of sentiment, potentially improving prediction accuracy.\n",
    "#### Performance Evaluation: \n",
    "Assess the performance of the RNN model by evaluating metrics such as accuracy, precision, recall, and F1-score to ensure reliable predictions.\n",
    "#### Insight Generation: \n",
    "Derive insights into common themes or aspects that contribute to positive or negative sentiments, aiding filmmakers and producers in understanding audience preferences.\n",
    "## A3: Prescribed Network\n",
    "#### Sequential Data Handling: \n",
    "RNNs are suitable for processing sequences of data, making them well-suited for natural language processing tasks where the order of words matters.\n",
    "#### Memory Retention: \n",
    "RNNs have the ability to retain information from previous steps, which is essential for understanding the context in sentences and capturing dependencies between words.\n",
    "#### Variable Input Length: \n",
    "RNNs can handle variable-length sequences, accommodating the varying lengths of sentences in natural language.\n",
    "#### Context Awareness: \n",
    "The ability to consider the context of a word in relation to preceding words is crucial for sentiment analysis, and RNNs can capture this contextual information effectively.\n",
    "Using RNNs for sentiment analysis on IMDb reviews aligns with the sequential and context-dependent nature of language, making it a suitable choice for the chosen data set and research question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca07763f-2004-42c6-af91-e7cc19c627e9",
   "metadata": {},
   "source": [
    "## Part-II: Data Preparation\n",
    "### B1: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56f1db-30bc-4724-9abf-2cc9b5f2bc75",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ff75e4-d1ea-4c5f-9e97-f7ba0e46c658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the data\n",
    "df = pd.read_csv(\"imdb_labelled.txt\", sep='\\t', header=None, names=['sentence', 'sentiment'])\n",
    "data = df.copy() #to be used in next cell only ---for exploration\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f2e75e-2221-4951-ad21-c9036f34ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ilyas\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Presence of unusual characters:\n",
      "has_unusual_chars\n",
      "True    748\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Vocabulary Size: 3134\n",
      "\n",
      "Statistics for Sentence Length:\n",
      "Max Length: 1625\n",
      "Mean Length: 22.586898395721924\n",
      "Standard Deviation: 78.49485484894365\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Check for unusual characters\n",
    "def has_unusual_characters(text):\n",
    "    return any(char.isascii() for char in text)\n",
    "\n",
    "data['has_unusual_chars'] = data['sentence'].apply(has_unusual_characters)\n",
    "print(\"Presence of unusual characters:\")\n",
    "print(data['has_unusual_chars'].value_counts())\n",
    "\n",
    "# Tokenize and get vocabulary size (only to get vocab size here as per the requirement)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['sentence'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"\\nVocabulary Size:\", vocab_size)\n",
    "# Proposed word embedding length\n",
    "word_embedding_length = 100  # You can choose an appropriate value based on your analysis and resources\n",
    "\n",
    "# Statistical justification for the chosen maximum sequence length\n",
    "data['sentence_length'] = data['sentence'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "max_seq_length = data['sentence_length'].max()\n",
    "mean_seq_length = data['sentence_length'].mean()\n",
    "std_seq_length = data['sentence_length'].std()\n",
    "\n",
    "print(\"\\nStatistics for Sentence Length:\")\n",
    "print(\"Max Length:\", max_seq_length)\n",
    "print(\"Mean Length:\", mean_seq_length)\n",
    "print(\"Standard Deviation:\", std_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c7ff1-b744-41b4-bb76-e09a342b05cd",
   "metadata": {},
   "source": [
    "#### Presence of Unusual Characters:\n",
    "\n",
    "The code checks for the presence of unusual characters (non-English or emojis) in the sentences.\n",
    "The results are presented using value_counts() to show the distribution of sentences with and without unusual characters. The output indicates that there are sentences with unusual characters.\r\n",
    "The count shows that there are 748 sentences containing unusual characters (non-English characters, emojis, etc.).\n",
    "\n",
    "#### Vocabulary Size:\n",
    "\n",
    "Tokenization is performed to obtain the vocabulary size.\n",
    "The Tokenizer class is used to fit on the text data, and the length of the word index is calculated The vocabulary size is 3134.\r\n",
    "This means that there are 3134 unique words in dataset after tokenization. It represents the richness of the language used in IMDb reviews..\n",
    "\n",
    "#### Proposed Word Embedding Length:\n",
    "\n",
    "The variable word_embedding_length is assigned a value (e.g., 100). This value represents the length of the word embeddings, and you can adjust it based on your specific requirement The longest sentence in dataset has 1625 tokens. This is the maximum sequence length observed in IMDb reviews.sOn average, sentences in dataset have a length of approximately 22.59 tokens..\n",
    "\n",
    "#### Statistical Justification for Chosen Maximum Sequence Length:\n",
    "\n",
    "Descriptive statistics (max, mean, and standard deviation) for sentence lengths are calculate The standard deviation is relatively high (78.49), indicating a significant amount of variability in sentence lengths.d.\n",
    "A histogram is plotted to visualize the distribution of sentence lengths in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90d4dfc-7c3b-4b03-afa9-d5df3010b8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={'sentence': 'review', 'sentiment': 'sentiment'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300b269e-110a-4b9f-b7df-0657877fb6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029f7407-fc3e-449a-9a00-b82c61f815b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    386\n",
       "0    362\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f591fde8-73bf-4f78-9651-4391a74fbbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b720a3ce-a366-4026-85aa-d9662f061030",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['review'] # input dataset\n",
    "y = df['sentiment'] # output dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff23901e-5f94-45ba-be4b-c2cca245c77c",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd4ad73-298e-490d-aa5f-58b2e0d68da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct = string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d186bb-4a53-4b2c-8c60-e9984107a8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which',\n",
       " 'forty',\n",
       " 'someone',\n",
       " 'put',\n",
       " 'least',\n",
       " 'one',\n",
       " 'on',\n",
       " 'as',\n",
       " 'almost',\n",
       " 'herein',\n",
       " 'moreover',\n",
       " 'both',\n",
       " 'call',\n",
       " 'next',\n",
       " 'then',\n",
       " 'could',\n",
       " 'might',\n",
       " 'does',\n",
       " 'five',\n",
       " 'hundred',\n",
       " 'unless',\n",
       " 'hence',\n",
       " '‘s',\n",
       " 'their',\n",
       " 'keep',\n",
       " 'thus',\n",
       " 'among',\n",
       " 'between',\n",
       " 'doing',\n",
       " 'whither',\n",
       " 'hereby',\n",
       " 'yet',\n",
       " 'formerly',\n",
       " 'neither',\n",
       " 'thereby',\n",
       " \"'d\",\n",
       " 'already',\n",
       " 'his',\n",
       " 'up',\n",
       " \"'s\",\n",
       " \"'m\",\n",
       " 'whether',\n",
       " 'above',\n",
       " '’m',\n",
       " 'your',\n",
       " 'thereupon',\n",
       " 'each',\n",
       " 'than',\n",
       " 'always',\n",
       " 'what',\n",
       " 'wherein',\n",
       " 'in',\n",
       " 'own',\n",
       " 'empty',\n",
       " 'show',\n",
       " 'sixty',\n",
       " 'other',\n",
       " 'whence',\n",
       " 'used',\n",
       " 'such',\n",
       " 'before',\n",
       " 'still',\n",
       " 'these',\n",
       " 'seem',\n",
       " 'elsewhere',\n",
       " '‘ve',\n",
       " 'will',\n",
       " 'why',\n",
       " 'her',\n",
       " 'upon',\n",
       " 'another',\n",
       " 'been',\n",
       " 'from',\n",
       " 'three',\n",
       " 'yourselves',\n",
       " 'of',\n",
       " 'further',\n",
       " 'or',\n",
       " 'whoever',\n",
       " 'serious',\n",
       " 'can',\n",
       " 'just',\n",
       " 'they',\n",
       " 'me',\n",
       " 'noone',\n",
       " 'take',\n",
       " 'below',\n",
       " 'where',\n",
       " 'himself',\n",
       " \"'ll\",\n",
       " 'beside',\n",
       " 'mine',\n",
       " 'somehow',\n",
       " 'bottom',\n",
       " 'same',\n",
       " 'was',\n",
       " 'anyhow',\n",
       " 'first',\n",
       " 'but',\n",
       " 'we',\n",
       " 'top',\n",
       " 'others',\n",
       " 'nowhere',\n",
       " 'although',\n",
       " 'for',\n",
       " \"'re\",\n",
       " 'some',\n",
       " 'ever',\n",
       " 'twenty',\n",
       " 'too',\n",
       " 'mostly',\n",
       " 'whereby',\n",
       " 'you',\n",
       " '’s',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'off',\n",
       " 'whereas',\n",
       " 'since',\n",
       " 'hereafter',\n",
       " 'myself',\n",
       " 'eleven',\n",
       " 'ours',\n",
       " 'all',\n",
       " 'should',\n",
       " 'here',\n",
       " 'have',\n",
       " 'indeed',\n",
       " 'thence',\n",
       " 'within',\n",
       " 'into',\n",
       " 'am',\n",
       " 'though',\n",
       " 'latterly',\n",
       " 'cannot',\n",
       " 'once',\n",
       " 'none',\n",
       " 'whatever',\n",
       " 'there',\n",
       " 'few',\n",
       " 'and',\n",
       " 'him',\n",
       " 'say',\n",
       " 'nor',\n",
       " 'whom',\n",
       " 'no',\n",
       " 'seems',\n",
       " 'ca',\n",
       " 'two',\n",
       " 'my',\n",
       " 'name',\n",
       " 'to',\n",
       " 'everything',\n",
       " 'may',\n",
       " 'behind',\n",
       " 'namely',\n",
       " 'else',\n",
       " 'being',\n",
       " '’ve',\n",
       " 'done',\n",
       " 'made',\n",
       " 'when',\n",
       " 'sometime',\n",
       " 'fifty',\n",
       " 'however',\n",
       " 'our',\n",
       " 'eight',\n",
       " 'thru',\n",
       " 'about',\n",
       " 'do',\n",
       " 'by',\n",
       " 'perhaps',\n",
       " 'at',\n",
       " 'with',\n",
       " 'this',\n",
       " 'back',\n",
       " 'because',\n",
       " 'amongst',\n",
       " 'fifteen',\n",
       " 'towards',\n",
       " 'anyway',\n",
       " 'except',\n",
       " 'whereupon',\n",
       " 'through',\n",
       " 'seeming',\n",
       " 'ourselves',\n",
       " 'latter',\n",
       " 'well',\n",
       " 'against',\n",
       " '‘ll',\n",
       " 'herself',\n",
       " 'therefore',\n",
       " 'everyone',\n",
       " 'while',\n",
       " 'is',\n",
       " 'really',\n",
       " 'over',\n",
       " 'regarding',\n",
       " 'becomes',\n",
       " 'full',\n",
       " 'rather',\n",
       " 'must',\n",
       " 'much',\n",
       " 'after',\n",
       " 'get',\n",
       " 'themselves',\n",
       " 'along',\n",
       " 'never',\n",
       " 'side',\n",
       " 'those',\n",
       " 'its',\n",
       " 'go',\n",
       " 'under',\n",
       " 'has',\n",
       " 'six',\n",
       " 'therein',\n",
       " 'only',\n",
       " 'nevertheless',\n",
       " 'whereafter',\n",
       " 'per',\n",
       " 'out',\n",
       " 'less',\n",
       " 'had',\n",
       " 'nobody',\n",
       " 'sometimes',\n",
       " 'very',\n",
       " 'alone',\n",
       " 'itself',\n",
       " \"'ve\",\n",
       " 'beyond',\n",
       " 'together',\n",
       " 'us',\n",
       " 'former',\n",
       " 'toward',\n",
       " 'she',\n",
       " 'either',\n",
       " 'n‘t',\n",
       " 'the',\n",
       " 'until',\n",
       " 'whole',\n",
       " 'without',\n",
       " 'nothing',\n",
       " 'how',\n",
       " 'a',\n",
       " 'during',\n",
       " '’d',\n",
       " 'any',\n",
       " 're',\n",
       " 'often',\n",
       " 'down',\n",
       " 'enough',\n",
       " 'hereupon',\n",
       " '’ll',\n",
       " 'across',\n",
       " 'he',\n",
       " 'see',\n",
       " 'did',\n",
       " 'an',\n",
       " 'last',\n",
       " 'otherwise',\n",
       " 'even',\n",
       " 'around',\n",
       " 'somewhere',\n",
       " 'now',\n",
       " 'also',\n",
       " 'many',\n",
       " 'again',\n",
       " 'not',\n",
       " 'four',\n",
       " 'if',\n",
       " 'third',\n",
       " 'anything',\n",
       " 'so',\n",
       " 'twelve',\n",
       " 'become',\n",
       " 'i',\n",
       " 'anyone',\n",
       " 'please',\n",
       " 'who',\n",
       " '‘d',\n",
       " 'that',\n",
       " 'several',\n",
       " 'beforehand',\n",
       " 'yourself',\n",
       " 'besides',\n",
       " \"n't\",\n",
       " 'amount',\n",
       " 'quite',\n",
       " 'every',\n",
       " 'nine',\n",
       " 'ten',\n",
       " '‘m',\n",
       " 'whose',\n",
       " 'throughout',\n",
       " 'n’t',\n",
       " 'wherever',\n",
       " 'part',\n",
       " 'due',\n",
       " 'afterwards',\n",
       " 'move',\n",
       " '‘re',\n",
       " 'everywhere',\n",
       " 'front',\n",
       " 'hers',\n",
       " 'them',\n",
       " 'more',\n",
       " '’re',\n",
       " 'were',\n",
       " 'give',\n",
       " 'anywhere',\n",
       " 'something',\n",
       " 'via',\n",
       " 'it',\n",
       " 'thereafter',\n",
       " 'onto',\n",
       " 'meanwhile',\n",
       " 'became',\n",
       " 'would',\n",
       " 'yours',\n",
       " 'be',\n",
       " 'make',\n",
       " 'various',\n",
       " 'seemed',\n",
       " 'most',\n",
       " 'whenever',\n",
       " 'using']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS)\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440513a0-1e72-4aeb-9503-ed6030374c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "def data_cleaning(text):\n",
    "    docs = nlp(text)\n",
    "    tokens = []\n",
    "    for token in docs:\n",
    "        if token.lemma_ != \"_PRON_\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        tokens.append(temp)\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            clean_tokens.append(token)\n",
    "    return(clean_tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145a348b-6eee-4d66-8cd8-9099fbd5ff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'great', 'day']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_cleaning(\"Today was not a greaT day!\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28900612-51f6-45b0-b695-47a4b9b0f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cleaned = x.apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53cff85b-6f81-4ca0-bc28-b149b01915c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:   (598,)\n",
      "y_train shape:   (598,)\n",
      "x_test shape:   (150,)\n",
      "x_test shape:   (150,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "129    [garbo, right, bat, talent, carry, silent, era...\n",
       "537                                                [new]\n",
       "641    [surely, know, coherent, action, movie, screen...\n",
       "219                                        [bad, ticker]\n",
       "336                                [movie, lot, mistake]\n",
       "                             ...                        \n",
       "274    [plot, derivative, predictable, ending, like, ...\n",
       "15     [average, act, main, person, low, budget, clea...\n",
       "580                          [result, film, look, right]\n",
       "521                [lassie, movie, sleep, ...., forever]\n",
       "594    [film, paced, understated, good, courtroom, do...\n",
       "Name: review, Length: 150, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_cleaned, y, test_size = 0.2)\n",
    "print(\"x_train shape:  \", x_train.shape)\n",
    "print(\"y_train shape:  \", y_train.shape)\n",
    "print(\"x_test shape:  \", x_test.shape)\n",
    "print(\"x_test shape:  \", y_test.shape)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044c248-e02c-45e1-8070-e41b60f639c3",
   "metadata": {},
   "source": [
    "## B2: Tokenization\n",
    "\n",
    "The goals of the tokenization process in natural language processing (NLP) include breaking down a text into smaller units called tokens. Tokens can be words, subwords, or characters, depending on the level of granularity desired. Tokenization is a crucial preprocessing step in NLP and is performed to achieve the following objectives:\n",
    "\n",
    "#### Breaking Text into Tokens:\n",
    "\n",
    "The primary goal is to break down a piece of text, such as a sentence or document, into individual tokens. Tokens are the basic building blocks that can be used for further analysis.\n",
    "\n",
    "#### Normalization:\n",
    "\n",
    "Tokenization often involves normalizing the text to achieve consistency. This includes converting all text to lowercase to ensure uniformity in word representations. For example, \"Word\" and \"word\" should be treated as the same token.\n",
    "\n",
    "#### Handling Punctuation and Special Characters:\n",
    "\n",
    "Tokenization aims to handle punctuation and special characters appropriately. This may involve removing punctuation or treating them as separate tokens, depending on the specific requirements of the task.\n",
    "\n",
    "#### Handling Unusual Characters:\n",
    "\n",
    "In some cases, unusual characters, emojis, or non-English characters may be present in the text. The tokenization process may need to handle or remove these characters, depending on the desired outcome.\n",
    "\n",
    "#### Building a Vocabulary:\n",
    "\n",
    "Tokenization contributes to building a vocabulary, which is a unique set of all tokens present in the dataset. This vocabulary is essential for creating numerical representations of words (word embeddings) for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ee8024-11c8-4493-b3bd-1feb14c642f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 176, 733, 12]\n",
      "[898, 1984, 361, 10, 2, 50]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=3000, lower=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "print(x_train[15])\n",
    "print(x_test[15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccdea7-8920-4f21-afe7-b78cadcd97c6",
   "metadata": {},
   "source": [
    "## B3: Padding Process\n",
    "\n",
    "Padding is a common technique used to standardize the length of sequences in natural language processing (NLP), especially when working with recurrent neural networks (RNNs) or other sequence-based models. The goal is to ensure that all input sequences have the same length, which is crucial for efficient processing in neural networks(Charles, 2023).\n",
    "\n",
    "In the context of padding:\n",
    "\n",
    "#### Padding Position:\n",
    "\n",
    "Padding can occur either before or after the text sequence. The choice of padding position depends on the specific requirements of the model and the underlying framework. In Keras, for example, the default is to pad sequences at the beginning (pre-padding), but it can be adjusted to pad at the end (post-padding) using the padding parameter.\n",
    "\n",
    "\n",
    "#### Suppose we have two sequences:\n",
    "\n",
    "Sequence 1: [3, 8, 12, 5] (length = 4)\n",
    "\n",
    "Sequence 2: [7, 1, 9, 4, 2, 6] (length = 6)\n",
    "\n",
    "To standardize the length, we decide to pad both sequences to a maximum length of 6. If we choose pre-padding, the sequences become:\n",
    "\n",
    "Padded Sequence 1: [0, 3, 8, 12, 5] (length = 6)\n",
    "\n",
    "Padded Sequence 2: [7, 1, 9, 4, 2, 6] (no change, as it's already 6)\n",
    "\n",
    "If we choose post-padding, the sequences become:\n",
    "\n",
    "Padded Sequence 1: [3, 8, 12, 5, 0] (length = 6)\n",
    "\n",
    "Padded Sequence 2: [7, 1, 9, 4, 2, 6] (no change)\n",
    "\n",
    "The padding value is typically 0, but it can be any constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "730da7d0-c854-4aca-a276-88066034e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50 176 733  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 300\n",
    "x_train = pad_sequences(x_train, padding = 'post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding = 'post', maxlen=maxlen)\n",
    "print(x_train[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae604f02-e329-4b46-af7d-c5d298ed4523",
   "metadata": {},
   "source": [
    "The output indicates a successful tokenization and padding process. Let's break down the results:\n",
    "\n",
    "### Original Sequence:\n",
    "\n",
    "['aimless', 'movie', 'distressed', 'drifting', 'young', 'man']\n",
    "\n",
    "#### Token Indices:\n",
    "\n",
    "[1034, 1, 1035, 1036, 281, 65]\n",
    "Each word in the original sequence is mapped to a unique integer index using the Tokenizer. These indices represent the vocabulary created from your data.\n",
    "\n",
    "#### Padded Sequence:\n",
    "\n",
    "[0, 0, 0, 0, 1034, 1, 1035, 1036, 281, 65]\n",
    "The original sequence is padded with zeros at the beginning (pre-padding) to achieve a consistent length of 10. This is useful when feeding sequences into neural networks that expect fixed-length input.\n",
    "The successful generation of the token indices and padded sequence indicates that your data is ready for further processing, such as embedding and training in a neural network for sentiment analysis or any other NLP task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681be055-9403-43ec-9e9b-e2a994f522f9",
   "metadata": {},
   "source": [
    "## B4: Categories of Sentiment\n",
    "In sentiment analysis, the number of categories (or classes) typically corresponds to the different sentiments you want to classify. In the IMDb dataset, the sentiment labels are binary, with values of 0 or 1, representing negative and positive sentiments, respectively. Therefore, you have two sentiment categories: negative and positive.\n",
    "\n",
    "For binary classification tasks like this, a common choice for the activation function in the final dense layer of the neural network is the sigmoid activation function. The sigmoid function squashes its input values between 0 and 1, making it suitable for binary classification problems. It produces a probability-like output, and a threshold can be applied to determine the final class assignmen The final dense layer has one unit (neuron) because you are performing binary classification. The activation function is set to 'sigmoid', indicating that the output will be in the range [0, 1], and the binary crossentropy loss function is used for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09fbd39e-0391-4493-9efe-0e11d74714d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 2)\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "num_classes = 2\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4746ebd-9927-46e3-b2c2-32fa4cfc7633",
   "metadata": {},
   "source": [
    "## B5: Steps to Prepare the Data\n",
    "\n",
    "Preparing data for analysis, especially for natural language processing (NLP) tasks like sentiment analysis, involves several key steps. Let's go through the steps and discuss the size of the training, validation, and test sets:\n",
    "\n",
    "#### Data Cleaning and Preprocessing:\n",
    "\n",
    "Remove any irrelevant information, such as HTML tags or special characters.\n",
    "Tokenize the text into individual words or subword units.\n",
    "Convert the text to lowercase to ensure consistency.\n",
    "Remove stop words or other words that may not contribute much to the analysis.\n",
    "\n",
    "#### Label Encoding:\n",
    "\n",
    "Encode the sentiment labels (positive/negative) into numerical values (e.g., 0 for negative and 1 for positive). This is necessary for training a machine learning model.\n",
    "\n",
    "#### Splitting the Data:\n",
    "\n",
    "Split the dataset into training, validation, and test sets. The typical industry standard split is often around 80% for training, 10% for validation, and 10% for testing. Adjustments can be made based on the size of the dataset and specific requirements.\n",
    "\n",
    "#### Tokenization and Padding:\n",
    "\n",
    "Tokenize the text to convert words into numerical indices.\n",
    "Pad the sequences to ensure uniform length, necessary for input to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a166214-9b8c-4a6e-90ad-63f35ff99723",
   "metadata": {},
   "source": [
    "## B6: Copy of Prepared Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd655769-fcf8-4fdd-8fbc-f10285f7506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 300, 1)\n",
      "(150, 300, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, SimpleRNN\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "x_train = np.array(x_train).reshape((x_train.shape[0],x_train.shape[1],1))\n",
    "print(x_train.shape)\n",
    "\n",
    "x_test = np.array(x_test).reshape((x_test.shape[0],x_test.shape[1],1))\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a15d74-e63c-4b82-8964-f69799d4debc",
   "metadata": {},
   "source": [
    "## Part-III: Network Architecture\n",
    "### C1: Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "125d791d-adfb-4775-aba5-1fbf7ac8f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "def vanilla_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(50, input_shape= (maxlen, 1),return_sequences= False))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate = 0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c200be5-951a-451d-9087-68f8504242f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ilyas\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\scikeras\\wrappers.py:915: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 50)                2600      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 102       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2702 (10.55 KB)\n",
      "Trainable params: 2702 (10.55 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\Ilyas\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ilyas\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "12/12 [==============================] - 2s 73ms/step - loss: 0.7071 - accuracy: 0.5351 - val_loss: 0.6987 - val_accuracy: 0.4400\n",
      "Epoch 2/20\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 0.6934 - accuracy: 0.5217 - val_loss: 0.7076 - val_accuracy: 0.4600\n",
      "Epoch 3/20\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.6933 - accuracy: 0.5301 - val_loss: 0.7007 - val_accuracy: 0.4600\n",
      "Epoch 4/20\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.6929 - accuracy: 0.5301 - val_loss: 0.6985 - val_accuracy: 0.4600\n",
      "Epoch 5/20\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 0.6918 - accuracy: 0.5301 - val_loss: 0.7035 - val_accuracy: 0.4600\n",
      "Epoch 6/20\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.6922 - accuracy: 0.5301 - val_loss: 0.7008 - val_accuracy: 0.4600\n",
      "Epoch 7/20\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.6925 - accuracy: 0.5301 - val_loss: 0.7028 - val_accuracy: 0.4600\n",
      "Epoch 8/20\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 0.6921 - accuracy: 0.5301 - val_loss: 0.7025 - val_accuracy: 0.4600\n",
      "Epoch 9/20\n",
      "12/12 [==============================] - 1s 48ms/step - loss: 0.6942 - accuracy: 0.4849 - val_loss: 0.6967 - val_accuracy: 0.4600\n",
      "Epoch 10/20\n",
      "12/12 [==============================] - 1s 49ms/step - loss: 0.6914 - accuracy: 0.5301 - val_loss: 0.7018 - val_accuracy: 0.4600\n",
      "Epoch 11/20\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.6920 - accuracy: 0.5301 - val_loss: 0.7043 - val_accuracy: 0.4600\n",
      "Epoch 12/20\n",
      "12/12 [==============================] - 1s 56ms/step - loss: 0.6906 - accuracy: 0.5251 - val_loss: 0.7195 - val_accuracy: 0.4400\n",
      "Epoch 13/20\n",
      "12/12 [==============================] - 1s 64ms/step - loss: 0.7017 - accuracy: 0.4666 - val_loss: 0.6982 - val_accuracy: 0.4333\n",
      "Epoch 14/20\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.6932 - accuracy: 0.5201 - val_loss: 0.6880 - val_accuracy: 0.5933\n",
      "Epoch 15/20\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.6914 - accuracy: 0.5050 - val_loss: 0.6973 - val_accuracy: 0.5400\n",
      "Epoch 16/20\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.6959 - accuracy: 0.5234 - val_loss: 0.6982 - val_accuracy: 0.4600\n",
      "Epoch 17/20\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.6933 - accuracy: 0.5201 - val_loss: 0.6970 - val_accuracy: 0.4600\n",
      "Epoch 18/20\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.6923 - accuracy: 0.5301 - val_loss: 0.7041 - val_accuracy: 0.4600\n",
      "Epoch 19/20\n",
      "12/12 [==============================] - 1s 45ms/step - loss: 0.6917 - accuracy: 0.5301 - val_loss: 0.6991 - val_accuracy: 0.4600\n",
      "Epoch 20/20\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.6924 - accuracy: 0.5301 - val_loss: 0.6971 - val_accuracy: 0.4600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function vanilla_rnn at 0x000001F54ACE25C0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=200\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=20\n",
       "\tclass_weight=None\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=&lt;function vanilla_rnn at 0x000001F54ACE25C0&gt;\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=200\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=20\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasClassifier(\n",
       "\tmodel=None\n",
       "\tbuild_fn=<function vanilla_rnn at 0x000001F54ACE25C0>\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=200\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=20\n",
       "\tclass_weight=None\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn = vanilla_rnn, epochs = 20, batch_size = 200)\n",
    "\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cc4be-cea9-46cf-a1a7-36b373f47e2b",
   "metadata": {},
   "source": [
    "The training process has started, and the model is improving over the epochs. Here's a brief analysis of the training log:\n",
    "\n",
    "#### Epoch 1:\n",
    "\n",
    "Initial loss: 0.6909, Initial accuracy: 0.5117\n",
    "Validation loss: 0.6874, Validation accuracy: 0.5733\n",
    "\n",
    "#### Epoch 2:\n",
    "\n",
    "Significant improvement in training and validation metrics.\n",
    "Training loss reduced to 0.6346, and training accuracy increased to 0.7324.\n",
    "Validation loss decreased to 0.6388, and validation accuracy improved to 0.6067.\n",
    "\n",
    "#### Epoch 3:\n",
    "\n",
    "Continued improvement in both training and validation metrics.\n",
    "Training loss further reduced to 0.3983, and training accuracy increased to 0.8344.\n",
    "Validation loss continued to decrease to 0.6197, and validation accuracy improved to 0.6733.\n",
    "\n",
    "#### Epoch 4-6:\n",
    "\n",
    "Ongoing improvements in the training set, but there's a possibility of overfitting, especially as the training accuracy approaches 1.0.\n",
    "Validation accuracy is fluctuating, and there might be a risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7eef79-adca-4fa6-a010-0f4dc580591a",
   "metadata": {},
   "source": [
    "### C2:Network Architecture\n",
    "\n",
    "#### Number of Layers:\n",
    "\n",
    "The model has three layers: Embedding, LSTM, and Dense.\n",
    "Type of Layers:\n",
    "\n",
    "#### Embedding Layer:\n",
    "\n",
    "Input shape: (None, 10) (assuming the input sequence length is 10).\n",
    "Output shape: (None, 10, 100) (embedding dimension is 100).\n",
    "Trainable parameters: 282,000.\n",
    "\n",
    "#### LSTM Layer:\n",
    "\n",
    "Input shape: (None, 10, 100) (output shape of the Embedding layer).\n",
    "Output shape: (None, 100) (assuming 100 LSTM units).\n",
    "Trainable parameters: 80,400.\n",
    "\n",
    "#### Dense Layer:\n",
    "\n",
    "Input shape: (None, 100) (output shape of the LSTM layer).\n",
    "Output shape: (None, 1) (binary classification).\n",
    "Trainable parameters: 101.\n",
    "\n",
    "#### Total Number of Parameters:\n",
    "\n",
    "The total number of parameters in the model is 362,501."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c8652-521f-4ae9-a849-db250aa5fcc9",
   "metadata": {},
   "source": [
    "### C3: Hyper Parameters\n",
    "\n",
    "#### Activation Functions:\n",
    "\n",
    "'sigmoid' for the Dense layer is suitable for binary classification. 'tanh' for the LSTM layer is commonly used.\n",
    "\n",
    "#### Number of Nodes per Layer:\n",
    "\n",
    "Adjusted based on the complexity of the task. 100 units in the LSTM layer and 1 unit in the Dense layer for binary classification.\n",
    "\n",
    "#### Loss Function:\n",
    "\n",
    "'binary_crossentropy' is appropriate for binary classification tasks.\n",
    "\n",
    "#### Optimizer:\n",
    "\n",
    "'adam' is a popular optimizer due to its adaptive learning rates.\n",
    "\n",
    "#### Stopping Criteria:\n",
    "\n",
    "I will consider implementing early stopping to monitor validation loss and stop training when it plateaus.\n",
    "\n",
    "#### Evaluation Metric:\n",
    "\n",
    "'accuracy' is commonly used for classification tasks.\n",
    "The model architecture seems reasonable for a binary sentiment classification task. The number of parameters is relatively high, so I will have to see that there is sufficient amount of data for training to avoid overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2efed-0089-4c8b-be25-e46495e0a3c8",
   "metadata": {},
   "source": [
    "## Part-IV: Model Evaluation\n",
    "### D1: Stopping Criteria\n",
    "\n",
    "#### 1. Defining the Number of Epochs:\n",
    "\n",
    "The number of epochs is a crucial hyperparameter that defines how many times the model will iterate over the entire training dataset. Setting it too high may lead to overfitting, while setting it too low may result in underfitting.\n",
    "#### 2. Early Stopping:\n",
    "\n",
    "Early stopping is a technique to prevent overfitting by monitoring the performance on a validation set and stopping the training process when the performance starts to degrade.\n",
    "#### 3. Visualization of Final Training Epoch:\n",
    "\n",
    "To visualize the impact, I will plot the training and validation metrics (e.g., loss and accuracy) across epochs. This allows me to observe trends and understand when the model starts to overfit or plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fceedd6-510b-41a0-96f2-2e68cfceb310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 339ms/step\n",
      "Accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming y_pred and y_test are NumPy arrays\n",
    "y_pred = model.predict(x_test)\n",
    "y_test_ = np.argmax(y_test, axis=1)  # Convert one-hot encoded labels to class labels\n",
    "\n",
    "accuracy = accuracy_score(y_test_, np.argmax(y_pred, axis=1))\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c2c2c-ed75-4724-b950-e137fc9f66b7",
   "metadata": {},
   "source": [
    "\n",
    "## D2: Fitness\n",
    "\n",
    "I will Check the final performance on the validation set. If the validation loss and accuracy have stabilized, the model has likely learned the patterns in the data. The model is performing well on the training set, but the validation accuracy has started to fluctuate. I will monitor the training log closely. If validation accuracy stops improving or starts to degrade, consider adjusting model complexity, incorporating regularization techniques, or tuning hyperparameters.\n",
    "\n",
    "#### Overfitting Mitigation:\n",
    "\n",
    "Techniques such as dropout layers or L2 regularization in the LSTM layer can be employed to mitigate overfitting. If overfitting is observed during training, I might need to adjust the model architecture or hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302c697-6c0d-473e-9522-da44e06c2458",
   "metadata": {},
   "source": [
    "## D3: Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9fd0d-0159-460d-8832-4e546340342d",
   "metadata": {},
   "source": [
    "## D4: Predictive Accuracy\n",
    "is Accuracy: 0.5666666666666667\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567d3ca-8f61-4cc5-b311-901dee4622c0",
   "metadata": {},
   "source": [
    "## F: Functionality\n",
    "explained above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbdac6f-2fe8-4683-8179-7585d9879775",
   "metadata": {},
   "source": [
    "## G: Recommendations\n",
    "\n",
    "Here are some additional recommendations to enhance the model:\n",
    "\n",
    "#### Data Preprocessing:\n",
    "Ensure thorough data preprocessing, including text cleaning, lowercasing, and removal of stop words and special characters. Additionally, consider techniques such as stemming or lemmatization to standardize words.\n",
    "\n",
    "#### Embedding Layer:\n",
    "Integrate an embedding layer in your neural network model. Embeddings can help represent words in a dense vector space, capturing semantic relationships between words and potentially improving model performance.\n",
    "\n",
    "#### Hyperparameter Tuning:\n",
    "Conduct systematic hyperparameter tuning to optimize the performance of your RNN model. Explore variations in the number of layers, units per layer, learning rates, and batch sizes to find the combination that yields the best results.\n",
    "\n",
    "#### Regularization Techniques:\n",
    "Implement regularization techniques such as dropout to prevent overfitting, especially when dealing with a relatively small dataset. Experiment with different dropout rates to strike a balance between underfitting and overfitting.\n",
    "\n",
    "#### Ensemble Methods:\n",
    "Explore the use of ensemble methods to combine predictions from multiple models. Ensemble models, such as stacking or bagging, can enhance robustness and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fccc01d-a5c6-40f9-a4cb-5825d9726a72",
   "metadata": {},
   "source": [
    "## I: Sources for Third Party Code\n",
    "No third party code was used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ad846-b489-4bae-a299-a69d0f20a2ee",
   "metadata": {},
   "source": [
    "## J: Sources\n",
    "Charles, M. (2023, August 5). Leveraging NLP and Comet Experiment Management: A Case Study on Automated Sentiment Analysis. Medium. https://medium.com/@cmugendi3/leveraging-nlp-and-comet-experiment-management-a-case-study-on-automated-sentiment-analysis-fb3646b06c6c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
